{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca3922f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model and preprocessors loaded successfully!\n",
      "\n",
      "ðŸ†• New customer data loaded:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>OnlineBackup</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>919-TMRGD</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>100</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>79.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n",
       "0  919-TMRGD  Female              0      No        Yes     100          Yes   \n",
       "\n",
       "  MultipleLines InternetService OnlineSecurity OnlineBackup DeviceProtection  \\\n",
       "0            No     Fiber optic             No           No               No   \n",
       "\n",
       "  TechSupport StreamingTV StreamingMovies        Contract PaperlessBilling  \\\n",
       "0          No         Yes              No  Month-to-month              Yes   \n",
       "\n",
       "      PaymentMethod  MonthlyCharges  \n",
       "0  Electronic check           79.35  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- TotalCharges\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 74\u001b[0m\n\u001b[0;32m     69\u001b[0m numerical_cols_present \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     70\u001b[0m     col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m numerical_cols_for_scaling \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m processed_data\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m     71\u001b[0m ]\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Apply the scaling transformation using the filtered list\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m processed_data[numerical_cols_present] \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnumerical_cols_present\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Drop customerID and TotalCharges (if it was added), using errors=\"ignore\" is safe\u001b[39;00m\n\u001b[0;32m     77\u001b[0m processed_data \u001b[38;5;241m=\u001b[39m processed_data\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomerID\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotalCharges\u001b[39m\u001b[38;5;124m\"\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:1075\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1072\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1074\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1075\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1087\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:2929\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2845\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalidate_data\u001b[39m(\n\u001b[0;32m   2846\u001b[0m     _estimator,\n\u001b[0;32m   2847\u001b[0m     \u001b[38;5;241m/\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2853\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m   2854\u001b[0m ):\n\u001b[0;32m   2855\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[0;32m   2856\u001b[0m \n\u001b[0;32m   2857\u001b[0m \u001b[38;5;124;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2927\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m   2928\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2929\u001b[0m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2930\u001b[0m     tags \u001b[38;5;241m=\u001b[39m get_tags(_estimator)\n\u001b[0;32m   2931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags\u001b[38;5;241m.\u001b[39mtarget_tags\u001b[38;5;241m.\u001b[39mrequired:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:2787\u001b[0m, in \u001b[0;36m_check_feature_names\u001b[1;34m(estimator, X, reset)\u001b[0m\n\u001b[0;32m   2784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m   2785\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 2787\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- TotalCharges\n"
     ]
    }
   ],
   "source": [
    "# ðŸ§  Predict_Churn.ipynb\n",
    "# Proper SHAP + prediction for churn, using pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import shap\n",
    "\n",
    "# -----------------------------\n",
    "# 1ï¸âƒ£ Load model and preprocessors\n",
    "# -----------------------------\n",
    "with open(\"best_model.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "with open(\"preprocessors.pkl\", \"rb\") as f:\n",
    "    preprocessors = pickle.load(f)\n",
    "    \n",
    "with open(\"best_cph_model.pkl\", \"rb\") as f:\n",
    "    saved = pickle.load(f)\n",
    "\n",
    "label_encoders = preprocessors[\"label_encoders\"]\n",
    "scaler = preprocessors[\"scaler\"]\n",
    "categorical_cols = preprocessors[\"categorical_cols\"]\n",
    "numerical_cols_for_scaling = preprocessors[\"numerical_cols_for_scaling\"]\n",
    "\n",
    "print(\"âœ… Model and preprocessors loaded successfully!\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2ï¸âƒ£ New customer data\n",
    "# -----------------------------\n",
    "new_data = pd.DataFrame([{\n",
    "    \"customerID\": \"919-TMRGD\",\n",
    "    \"gender\": \"Female\",\n",
    "    \"SeniorCitizen\": 0,\n",
    "    \"Partner\": \"No\",\n",
    "    \"Dependents\": \"Yes\",\n",
    "    \"tenure\": 100,\n",
    "    \"PhoneService\": \"Yes\",\n",
    "    \"MultipleLines\": \"No\",\n",
    "    \"InternetService\": \"Fiber optic\",\n",
    "    \"OnlineSecurity\": \"No\",\n",
    "    \"OnlineBackup\": \"No\",\n",
    "    \"DeviceProtection\": \"No\",\n",
    "    \"TechSupport\": \"No\",\n",
    "    \"StreamingTV\": \"Yes\",\n",
    "    \"StreamingMovies\": \"No\",\n",
    "    \"Contract\": \"Month-to-month\",\n",
    "    \"PaperlessBilling\": \"Yes\",\n",
    "    \"PaymentMethod\": \"Electronic check\",\n",
    "    \"MonthlyCharges\": 79.35\n",
    "}])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nðŸ†• New customer data loaded:\")\n",
    "display(new_data)\n",
    "\n",
    "# -----------------------------\n",
    "# 3ï¸âƒ£ Preprocess new data (same as training)\n",
    "# -----------------------------\n",
    "processed_data = new_data.copy()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in processed_data.columns:\n",
    "        le = label_encoders[col]\n",
    "        processed_data[col] = processed_data[col].apply(lambda x: x if x in le.classes_ else le.classes_[0])\n",
    "        processed_data[col] = le.transform(processed_data[col])\n",
    "\n",
    "processed_data[numerical_cols_for_scaling] = scaler.transform(processed_data[numerical_cols_for_scaling])\n",
    "processed_data = processed_data.drop(columns=[\"customerID\"], errors=\"ignore\")\n",
    "\n",
    "print(\"\\nâœ… Preprocessing complete:\")\n",
    "display(processed_data)\n",
    "\n",
    "# -----------------------------\n",
    "# 4ï¸âƒ£ Predict churn\n",
    "# -----------------------------\n",
    "prediction = model.predict(processed_data)[0]\n",
    "prediction_label = \"Churn\" if prediction == 1 else \"No Churn\"\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Predicted Result: {prediction_label}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5ï¸âƒ£ SHAP Explanation (fixed version)\n",
    "# -----------------------------\n",
    "# Create a background sample for SHAP\n",
    "# Use 100 rows from processed_X.csv if available (as background reference)\n",
    "try:\n",
    "    background = pd.read_csv(\"data/processed_X.csv\").sample(5000, random_state=42)\n",
    "except:\n",
    "    background = processed_data.copy()  # fallback if not available\n",
    "\n",
    "# Create SHAP explainer using pipeline and background\n",
    "explainer = shap.Explainer(model.predict, background)\n",
    "shap_values = explainer(processed_data)\n",
    "\n",
    "# Identify top features contributing to churn decision\n",
    "shap_df = pd.DataFrame({\n",
    "    \"feature\": processed_data.columns,\n",
    "    \"shap_value\": shap_values.values[0]\n",
    "}).sort_values(by=\"shap_value\", key=abs, ascending=False)\n",
    "\n",
    "top_features = shap_df.head(3)\n",
    "\n",
    "print(\"\\nðŸ”¥ Top Features Influencing Prediction:\")\n",
    "display(top_features)\n",
    "\n",
    "# Visualize detailed contribution\n",
    "shap.plots.waterfall(shap_values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ed2791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# surv_func = cph.predict_survival_function(new_data)\n",
    "# surv_func.plot()\n",
    "# plt.title(\"Predicted Survival Curve for New Customer\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26de1ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: preprocessing during training\n",
    "# train_encoded = pd.get_dummies(train, drop_first=False)\n",
    "\n",
    "# # Fit model\n",
    "# cph.fit(train_encoded, duration_col=\"duration\", event_col=\"event\")\n",
    "\n",
    "# # Now for new data:\n",
    "# new_data = pd.DataFrame({\n",
    "#     \"gender\": [\"Male\"],\n",
    "#     \"SeniorCitizen\": [\"Yes\"],\n",
    "#     \"Partner\": [\"Yes\"],\n",
    "#     \"Dependents\": [\"No\"],\n",
    "#     \"InternetService\": [\"Fiber optic\"],\n",
    "#     \"Contract\": [\"One year\"],\n",
    "#     \"PaymentMethod\": [\"Electronic check\"],\n",
    "#     \"MonthlyCharges\": [80],\n",
    "#     \"TotalCharges\": [3000],\n",
    "#     \"tenure\": [24]\n",
    "# })\n",
    "\n",
    "# # Encode it the same way\n",
    "# new_data_encoded = pd.get_dummies(new_data, drop_first=False)\n",
    "\n",
    "# # Reindex to match training columns\n",
    "# new_data_encoded = new_data_encoded.reindex(columns=train_encoded.drop(columns=[\"duration\",\"event\"]).columns, fill_value=0)\n",
    "\n",
    "# # Predict\n",
    "# surv_func = cph.predict_survival_function(new_data_encoded)\n",
    "# surv_func.plot()\n",
    "# plt.title(\"Predicted Survival Curve for New Customer\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab34216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cph = saved[\"model\"]\n",
    "scaler = saved[\"scaler\"]\n",
    "train_columns = saved[\"columns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154268f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode new data like training data\n",
    "new_data_enc = pd.get_dummies(new_data)\n",
    "new_data_enc = new_data_enc.reindex(columns=train_columns, fill_value=0)\n",
    "\n",
    "# Apply scaling only to numeric columns used during training\n",
    "num_cols = scaler.feature_names_in_  # available in sklearn >= 1.0\n",
    "new_data_enc[num_cols] = scaler.transform(new_data_enc[num_cols])\n",
    "\n",
    "# Predict survival function\n",
    "surv_func = cph.predict_survival_function(new_data_enc)\n",
    "surv_func.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee44ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
